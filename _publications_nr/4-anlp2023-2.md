---
title: "敵対的学習を用いた知識蒸留への中間層蒸留と対照学習の導入"
collection: publications_nr
permalink: /publication/2023/3/4-anlp2023-2
excerpt: '知識蒸留(KD)とは，大規模なニューラルネットワークを圧縮する手法の一つである．言語モデル向けKDの中で最高性能の手法は，敵対的学習に中間層出力と対照学習を導入したCILDAと呼ばれる手法である．CILDAの学習は最大化ステップと最小化ステップに分かれているが，中間層出力と対照学習は最大化ステップでのみ活用されている．本研究では，最小化ステップに中間層蒸留と対照学習を導入し，性能を向上させることを目指した．しかし，既存手法に対して有意な差は確認できなかったため，原因分析のためにCILDA単体の再現実験を行ったところ，先行研究の主張とは異なり，GLUEにおける複数のタスクでCILDAがそれ以前の手法の性能を上回らないという結果を得た．'
date: 2023/3/4
venue: '言語処理学会第29回年次大会発表論文集'
paperurl: 'https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q3-2.pdf'
citation: '鈴木偉士, 山田寛章, 徳永健伸. 敵対的学習を用いた知識蒸留への中間層蒸留と対照学習の導入. <i>言語処理学会第29回年次大会発表論文集</i>, pp. 783-788, 2023年3月.'
---
**Abstract**   
知識蒸留(KD)とは，大規模なニューラルネットワークを圧縮する手法の一つである．言語モデル向けKDの中で最高性能の手法は，敵対的学習に中間層出力と対照学習を導入したCILDAと呼ばれる手法である．CILDAの学習は最大化ステップと最小化ステップに分かれているが，中間層出力と対照学習は最大化ステップでのみ活用されている．本研究では，最小化ステップに中間層蒸留と対照学習を導入し，性能を向上させることを目指した．しかし，既存手法に対して有意な差は確認できなかったため，原因分析のためにCILDA単体の再現実験を行ったところ，先行研究の主張とは異なり，GLUEにおける複数のタスクでCILDAがそれ以前の手法の性能を上回らないという結果を得た．

**Recommended citation:**   
鈴木偉士, 山田寛章, 徳永健伸. 敵対的学習を用いた知識蒸留への中間層蒸留と対照学習の導入. <i>言語処理学会第29回年次大会発表論文集</i>, pp. 783-788, 2023年3月.

<a href='https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q3-2.pdf'>Download paper here</a>
